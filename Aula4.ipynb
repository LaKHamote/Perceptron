{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccb748d6-e6b8-4737-942e-5a5d569ef0f1",
   "metadata": {},
   "source": [
    "# Regressão Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb34d24-47e3-453b-ac65-a300e7b46826",
   "metadata": {},
   "source": [
    "## 1. Definições"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a399af6-259a-45ee-bf39-69d8248659b4",
   "metadata": {},
   "source": [
    "Por regressão linear estamos falando em estimar uma variável $y$ a partir de uma série de características, sob a hipótese que existe uma relação aproximadamente linear entre saída e características. Por exemplo, estimar o preço de uma casa a partir de área construída, localização geográfica, etc. Partimos da equação \n",
    "\\begin{equation}\n",
    "\\hat{y}=w_{1}x_{1}+w_{2}x_{2}+\\cdots +w_{d}x_{D}+b,\n",
    "\\end{equation}\n",
    "onde $\\hat{y}$ é a estimativa da variável $y$ dadas as características $x_{i}$, $w_{i}$ são parâmetros do modelo a serem determinados, e $b$ é um valor de viés (\"bias\") para não nos restringirmos a retas e planos que passam pela origem. \n",
    "\n",
    "Generalizando em notação vetorial, sendo $\\vec{x}$ o vetor de características e $\\vec{w}$ o vetor de parâmetros,\n",
    "\\begin{equation}\n",
    "\\hat{y}=\\vec{w}^{T}\\vec{x}+b,\n",
    "\\end{equation}\n",
    "onde T indica o vetor transposto. \n",
    "\n",
    "Finalmente, frequentemente em regressão temos uma grande quantidade de exemplos para ajustar o modelo. Poderíamos aplicar a equação acima $N$ vezes para $N$ exemplos, mas é mais conveniente imaginar todos os $N$ exemplos para $x$ em uma matriz $\\matrix{X}$, de tamanho $N\\times d$, ou seja, com um exemplo de parâmetros $x$ em cada linha. Neste caso, \n",
    "\\begin{equation}\n",
    "\\hat{y}=\\matrix{X}\\matrix{W}+b,\n",
    "\\end{equation}\n",
    "onde a matriz $\\matrix{W}$ é obtida pela repetição do mesmo vetor de parâmetros $\\vec{w}$ em N colunas. \n",
    "\n",
    "Obs: Lembre-se do \"broadcasting\" em Python. Você pode fazer o produto da matriz X pelo vetor W e ele será corretamente interpretado: o vetor W será repetido em N colunas, e o resultado do produto (que é uma matriz NxN, com a estimativa para cada exemplo repetido N vezes), será re-interpetado como um vetor de tamanho N. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c4e30-ebf2-4dea-bbae-cfa1c4ffa132",
   "metadata": {},
   "source": [
    "## 2. Função de custo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26306baf-afb0-4235-ab31-6d7d5639264a",
   "metadata": {},
   "source": [
    "Queremos encontrar os melhores valores $w$ e $b$, mas em qual critério? Geralmente queremos minimizar alguma função de custo (\"loss function\"). Uma escolha muito comum para o caso da regressão é a soma dos erros quadráticos. Em outras palavras, queremos minimizar a norma L2 da diferença entre valores estimados e reais. \n",
    "\\begin{equation}\n",
    "L(\\vec{w},b)=\\frac{1}{N}\\sum_{i=1}^{N}\\left( \\vec{w}^{T}\\vec{x}^{(i)}-y^{(i)}\\right)^{2},\n",
    "\\end{equation}\n",
    "onde o sobrescrito $(i)$ faz referência ao i-ésimo exemplo. \n",
    "\n",
    "Note que multiplicar L por uma constante não altera o problema. Daí ser normal a divisão por N (tornando L uma média do erro quadrático). Quando formos derivar esta expressão, é conveniente colocar um fator $\\frac{1}{2}$ para cancelar o 2 do expoente. \n",
    "\n",
    "Nestas condições, o problema se torna encontrar valores ótimos $\\vec{w}^{*}$ e $b^{*}$ segundo\n",
    "\\begin{equation}\n",
    "\\vec{w}^{*}, b^{*} = \\text{argmin}\\ L(\\vec{w},b)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae2aff-c307-42e5-900d-52231b93dfe9",
   "metadata": {},
   "source": [
    "## 3. Solução Analítica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa7e23-b7f3-4558-831f-35c3c4cd413b",
   "metadata": {},
   "source": [
    "Este problema de minimização tem solução única (se fizermos o requisito adicional que $\\vec{w}$ tenha norma mínima. A demonstração pode ser encontrada em livros de álgebra linear. Para simplificar as coisas (como já avisamos que faríamos) vamos adicionar $b$ ao final do vetor $w$ (que passaará a ter tamanho $d+1$ e acrescentar uma parâmetro fictício 1 ao final de cada exemplo em $\\matrix{X}$. Neste caso, passamos a ter $\\hat{y}=\\matrix{X}\\matrix{W}$. Com essas modificações, a solução é \n",
    "\\begin{equation}\n",
    "\\vec{w}^{*} = \\left(\\matrix{X}^{T}\\matrix{X}\\right)^{-1}\\matrix{X}^{T}\\vec{y},\n",
    "\\end{equation}\n",
    "se a matriz $\\matrix{X}^{T}\\matrix{X}$ é inversível. Para que seja inversível, os exemplos não podem ser linearmente dependentes. \n",
    "\n",
    "Mesmo sem demonstração, algumas observações podem ser esclarecedoras. Primeiro observe que, se $y$ e $x$ realmente tivessem um relacionamento linear, eu só precisaria de $d$ exemplos diferentes. A matriz $\\matrix{X}$ seria quadrada de tamanho $d$, e tudo se resumiria a um conjunto de $N$ equações e $N$ incógnitas, com a solução dada por $\\vec{w}^{*}=\\matrix{X}^{-1}\\vec{y}$. Assim, a matriz $\\left(\\matrix{X}^{T}\\matrix{X}\\right)^{-1}\\matrix{X}^{T}$ está agindo como a \"inversa\" da matriz retangular $\\matrix{X}$, que a rigor não pode ter inversa. Por isso, esta matriz é chamada pseudo-inversa (ou inversa de Moore-Penrose). \n",
    "\n",
    "Ocorre que o relacionamento não é, de fato, linear. Ou então, este modelo está desprezando a influência de outras variáveis, o que faz a resposta estar ligeiramente errada para cada exemplo (o que será visto como um ruído no nosso modelo). Assim, eu reúno um grande número de exemplos e aplico a equação acima. \n",
    "\n",
    "Para problemas verdadeiramente de interesse em IA, o modelo linear é muito limitado, e assim que colocarmos uma não-linearidade deixamos de ter esta solução analítica. Mesmo que ela exista, pode ser impraticável de calcular em um único passo (se houver milhões de exemplos). Precisamos ver como minimizar iterativamente o erro quadrático. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52dbac-93dc-4134-b9d5-fe56ffd07cb8",
   "metadata": {},
   "source": [
    "## 4. Otimização por Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efeac65-96f1-419b-af83-c2b7b13f26be",
   "metadata": {},
   "source": [
    "A alternativa quando é impossível (analítica ou computacionalmente) calcular o valor ótimo na forma acima, é usar a descida por gradiente que vimos na aula passada. Em particular, neste método, apresenta-se um exemplo $\\vec{x}^{(i)}$ ao modelo, obtendo-se a estimativa $\\hat{y}^{(i)}$, e os parâmetros são atualizados na forma de um ajuste $\\vec{\\Delta w}$, calculado de acordo com \n",
    "\\begin{equation}\n",
    "\\vec{\\Delta w}=-\\eta\\  \\vec{x}^{(i)}\\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\Delta b = -\\eta\\ \\left( \\hat{y}^{(i)} - y^{(i)} \\right),\n",
    "\\end{equation}\n",
    "onde $\\eta$ é uma constante, frequentemente denominada \"de aprendizado\" na área de redes neurais. \n",
    "\n",
    "A atualização acima, vinda de considerações de derivada, tem um resultado intuitivamente correto. Observe que a atualização depende do produto da entrada e o erro, de forma tal que, se a saída deveria ser maior do que foi, e a entrada é positivo, a conexão com aquela entrada é fortalecida. Veja também que está implícita uma forma de correlação entre entrada e saída, não muito longe do que Hebb imaginou para a adaptação de conexões sinápticas. \n",
    "\n",
    "Note também como, mais uma vez, podemos simplificar as coisas considerando o \"bias\" b como simplesmente um peso adicional ligado a uma entrada fictícia que vale sempre 1. \n",
    "\n",
    "Este tipo de atualização é chamada \"online\" ou \"estocástica\". Uma alternativa é calcular a correção dos parâmetros a partir de todos os exemplos, segundo\n",
    "\\begin{equation}\n",
    "\\vec{\\Delta w}=-\\frac{\\eta}{N}\\  \\sum_{i=1}^{N}\\vec{x}^{(i)}\\left( \\hat{y}^{(i)} - y^{(i)} \\right)\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\Delta b = -\\frac{\\eta}{N}\\  \\sum_{i=1}^{N}\\left( \\hat{y}^{(i)} - y^{(i)} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Esta é a chamada atualização em batelada (\"batch\"). Observe que, como a atualização tende a ser maior em módulo neste caso (porque somamos a atualização devida ao erro calculado em todos os exemplos), dividimo-la por $N$. A rigor, isto é um preciosismo, porque o valor de $\\eta$ é geralmente definido por tentativa. Serve para nos lembrar, no entanto, que em geral a constante de aprendizado deve ser menor neste caso. \n",
    "\n",
    "Nos problemas complexos em que aplicamos aprendizado de máquina hoje, com bilhões de parâmetros e milhões de exemplos, a solução que se mostrou mais útil é intermediária entre esses dois extremos. Os $N$ exemplos são separados em \"minibatches\" $\\mathscr{B}$, de tamanho menor que N. O processamento e atualização são feitos sequencialmente por minibatch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a8a9bf-6989-4f0a-addd-03672eadd1d1",
   "metadata": {},
   "source": [
    "## 5. Relação com Neurônio Artificial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08b82770-1f5f-4c60-8714-4c372f0f5d4f",
   "metadata": {},
   "source": [
    "Vimos que um primeiro modelo simplificado de neurônio efetua uma soma ponderada de entradas e a passa por uma função do tipo limiar (\"threshold\"). Se retiramos esta não-linearidade, observe que temos algo indistinguível do modelo de regressão linear que discutimos aqui. \n"
   ]
  },
  {
   "attachments": {
    "7534baa5-5670-4c3d-8e9f-799158329620.gif": {
     "image/gif": "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"
    }
   },
   "cell_type": "markdown",
   "id": "bc270940-57ee-4066-a18a-0271807cc0c1",
   "metadata": {},
   "source": [
    "![singleneuron.gif](attachment:7534baa5-5670-4c3d-8e9f-799158329620.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabc4dff-8139-43ce-9b0d-0d9e86104ff8",
   "metadata": {},
   "source": [
    "Assim, observe que este nosso primeiro modelo \"inteligente\" pode ser compreendido perfeitamente como vindo do campo da probabilidade e estatística. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03483093-7afb-4543-b2dd-a9a1e05455b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
